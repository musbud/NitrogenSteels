---
#title: "NitrogenSteels"
---

# High nitrogen alloyed stainless steels

Nitrogen-alloyed stainless steels are special grades that are particularly used in the aerospace, energy, and medical sectors. Nitrogen, as a key alloying element, provides the material with high tensile strength, improves corrosion resistance, and stabilizes the formation of an austenitic microstructure.

However, nitrogen has a low solubility in molten steel or iron (maximum approximately 0.04 wt-%). Higher nitrogen levels can be achieved by alloying with elements such as chromium, manganese, or molybdenum, which increase the solubility of nitrogen in molten steel. Another approach to retain nitrogen in molten steel is to increase the nitrogen partial pressure in the furnace. [Energietechnik Essen GmbH](https://www.gmh-gruppe.de/standorte/energietechnik-essen-gmbh/) (ETE) operates a **p**ressure-**e**lectro**s**lag-**r**emelting (PESR) furnace to enhance the nitrogen content in stainless steels.

# Analysis of process parameters in PESR

The production of nitrogen-alloyed stainless steels requires in-depth knowledge of the entire process. Understanding the relationship between process parameters and nitrogen content is crucial for manufacturing high-quality steel products.

For an initial estimation of the relationships between process parameters and nitrogen content, we use a correlation matrix (see @fig-correlationmatrix).

According to the correlation matrix, Variable_3 shows the strongest positive correlation with the nitrogen content in the steel. This means that the higher Variable_3 is, the higher the nitrogen content in the grade. Conversely, as Variable_4 increases, the nitrogen content decreases; this parameter exhibits a negative correlation with nitrogen. Variable_5 shows almost no correlation with nitrogen, suggesting it has little to no effect on the nitrogen content of the steel.

```{r}
#| warning: FALSE
#| echo: False
#| message: False
#| label: fig-correlationmatrix
#| fig-cap: Correlationmatrix
#| fig-align: "center"
#| fig-pos: "H"
#| fig-width: 10
#| fig-height: 10

# load libraries
library(tidyverse)
library(scales)
library(ggcorrplot)
library(caret)

# load data
data.cor <- readRDS("data.cor.rds") # load your data set

# create correlation matrix
cor_matrix <- cor(data.cor[, c("N", "parameter_1", "parameter_2", "parameter_3", "parameter_4", "parameter_5")], use = "pairwise.complete.obs")  # Ges-Druck viele Na"s


# Plot correlation matrix
ggcorrplot(cor_matrix)
```

However, correlation does not imply causation. The next step will be to determine the significance of these results using an F-test for the multiple regression model.

# Multiple linear regression (MLR)

In regression analysis, the [**ANOVA F-test**](https://www.geeksforgeeks.org/r-language/anova-test-in-r-programming/) is used to evaluate whether the model respectively the model-parameters significantly explains the variability in the dependent variable, we will use a significance level of 0.05.

Based on the F-statistic, the model (with a p-value \< 2.2e-16) and its parameters are statistically significant.

```{r}
#| warning: FALSE
#| echo: False
#| message: False

# divide data set in trainings- and test data
# index <- createDataPartition(data.schleuse.cor$N, p = .70, list = FALSE)
# train.data <- data.schleuse.cor[index, ]
# test.data <- data.schleuse.cor[-index, ]
# dim(train.data)
# dim(test.data)

# save train- and test data
# saveRDS(train.data, "train.data.rds")
# saveRDS(test.data, "test.data.rds")

# load data
train.data <- readRDS("train.data.rds") # use your own train data
test.data <- readRDS("test.data.rds") # use your own test data

# perform multiple linear regression
options(contrasts = c("contr.sum", "contr.poly"))

set.seed(100)
reg.mod  <-  lm(N ~  scale(parameter_1) + scale(parameter_2) + scale(parameter_3) + scale(parameter_4) + scale(parameter_5), train.data)

# summirise the results
summary(reg.mod)

# prediction
pred.reg <- predict(reg.mod, test.data)
test.data$predict <- pred.reg

# R^2 (Test data!!)
# (R2 = 1 - sum((test.data$N - test.data$predict)^2) / sum((test.data$N - mean(test.data$N))^2))

# ------------------- ANHANG -------------------
# reg.mod  <-  lm(N ~  parameter_1 + parameter_2 + parameter_3 + parameter_4 + parameter_5, train.data)

# reg.mod  <-  lm(N ~  parameter_1 * parameter_2 * parameter_3 * parameter_4 * parameter_5, train.data)

# reg.mod  <-  lm(N ~  scale(parameter_1) * scale(parameter_2) * scale(parameter_3) * scale(parameter_4) * scale(parameter_5), train.data)
```

## Actual vs. estimated nitrogen content according to MLR

```{r}
#| warning: FALSE
#| echo: False
#| message: False
#| label: fig-ActualVsEstimatedMLR
#| fig-cap: Actual vs. estimated nitrogen content according to mlr 
#| fig-align: "center"
#| fig-pos: "H"
#| fig-width: 10
#| fig-height: 7


# Plot Messung gegen Schätzung

ggplot(test.data, aes(x = predict, y = N)) +
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  scale_x_continuous(breaks = pretty_breaks(n = 10), limits = c(0.3, 0.46)) +
  scale_y_continuous(breaks = pretty_breaks(n = 10), limits = c(0.3, 0.46)) +
  theme_bw(base_size = 25) + annotate("text", x = 0.43, y = 0.32, label = "R^2 == 0.41", size = 8, parse = T) +
  labs(x='estimated nitrogen content [wt.%]', y='actual nitrogen content [wt.%]', title='Multiple lineare Regression')


```

## Feature importance according to MLR

@fig-EffectLinearRegression shows the scaled coefficients of the MLR model. Variable_3 has the greatest influence on nitrogen solubility: the higher this parameter, the higher the nitrogen content in the molten steel. Conversely, the higher Variable_4 and Variable_5, the lower the solubility of nitrogen in the molten steel.

```{r}
#| warning: FALSE
#| echo: False
#| message: False
#| label: fig-EffectLinearRegression
#| fig-cap: Impact of diverse parameters on the nitrogen content in molten steel
#| fig-align: "center"
#| fig-pos: "H"
#| fig-width: 10
#| fig-height: 9

# extract model-coefficients
koeff <- data.frame(reg.mod[["coefficients"]])
# make row to cols and rename col
koeff <- koeff %>% rownames_to_column("Parameter") %>% rename("Model coefficients" = "reg.mod...coefficients...")
# remove not significant or intercept
koeff <- koeff %>% filter(!row_number() %in% c(1))

# Plot
#png("KoeffRegModOhneInteraktion.png", width = 1500, height = 800)
koeff %>%
  arrange(desc(abs(`Model coefficients`))) %>% # First sort by val. This sort the dataframe but NOT the factor levels
  mutate(Parameter = factor(Parameter, levels = Parameter)) %>% # This trick update the factor levels
  ggplot(aes(x = Parameter, y = `Model coefficients`)) + geom_point(size = 6) + xlab("") +
  scale_y_continuous(name = "coefficients of the model",  breaks=pretty_breaks(n=10)) +
  theme_bw(base_size = 25) +
  theme(
    #title = element_text(size = 16),
  #                  axis.title.x = element_text(size = 16),
                    #axis.text.x = element_text(size = 20,angle=45,vjust=0.5),
                    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
  #                  axis.title.y = element_text(size = 15,vjust=1.7),
  #                  axis.text.y = element_text(size = 16),
  #                  legend.title = element_text(size = 16),
  #                  legend.text = element_text(size = 16),
                    panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                    strip.text = element_text(size = 16))

#dev.off()

```

However, due to the relatively small R² value, the MLR model cannot adequately capture the deviations in the response variable (nitrogen content). Let us apply the eXtreme Gradient Boosting algorithm using the XGBoost package in R to determine whether it improves the model performance.

# XGBoost

Below are the optimal cross-validation results and tuning parameters (called *hyperparameters*, for more information visit [TuningOfParameters](https://carpentries-incubator.github.io/r-ml-tabular-data/06-Exploration/index.html)) for XGBoost method. Thanks to this hyperparameters the model will optimize the estimation of nitrogen content respectively the reliability of the XGBoost-model.

```{r}
#| warning: FALSE
#| echo: False
#| message: False

# load library
library(xgboost)
library(SHAPforxgboost)
library(doParallel)

# defeine trainings and test data - define variables / predictors 
x_train <- data.matrix(train.data[, c("parameter_1", "parameter_2", "parameter_3", "parameter_4", "parameter_5")])


x_test <- data.matrix(test.data[, c("parameter_1", "parameter_2", "parameter_3", "parameter_4", "parameter_5")])
# Defining the Target
y_train <- train.data$N
y_test <- test.data$N

#define final training and testing sets
xgb_train = xgb.DMatrix(data = x_train, label = y_train)
xgb_test = xgb.DMatrix(data = x_test, label = y_test)

#define watchlist
watchlist = list(train = xgb_train, test = xgb_test)



# -------------------------- Search for optimal hyparparameters with xgboost package --------------------------

############ !!!!!!!!!!!!! With xgb.cv() command !!!!!!!!!!!!!
############ !!!!!!!!!!!!! With xgb.cv() command !!!!!!!!!!!!!

# see for more information https://carpentries-incubator.github.io/r-ml-tabular-data/06-Exploration/index.html

# paramDF <- expand.grid(
#   nrounds = c(100, 500),
#   max_depth =c(3, 6, 8),# 6 is default value
#   learning_rate = c(0.001, 0.01, 0.1), #eta, 0.3 is default value
#   gamma = c(0, 0.1, 0.5),# 0 is default value
#   colsample_bytree = c(0.6, 0.8, 1),# 1 is default value
#   min_child_weight =c(1, 3, 5), # 1 is default
#   subsample = c(1),# 1 is default
#   reg_lambda = c(0.1, 1, 10), #lambda default is 1
#   reg_alpha = c(0, 1, 10)# 0 ist default
#   )
  
# paramList <- lapply(split(paramDF, 1:nrow(paramDF)), as.list)
# bestResults <- tibble()

# Set up parallel backend - parallel computing occurs automatically
# set.seed(100)
# pb <- txtProgressBar(style = 3)
# for(i in seq(length(paramList))) {
#   rwCV <- xgb.cv(params = paramList[[i]],
#                      data = xgb_train,
#                      nfold = 5,
#                      nrounds = paramDF$nrounds,
#                      early_stopping_rounds = 10,
#                      verbose = FALSE)
#   bestResults <- bestResults %>%
#     bind_rows(rwCV$evaluation_log[rwCV$best_iteration])
#   gc()  # Free unused memory after each loop iteration
#   setTxtProgressBar(pb, i/length(paramList))
# }
# below: perform after for-loop!!!
# close(pb) # done with the progress bar

########################## In "rwCV" / "rwCV$evaluation_log" sind werte von der letzten Berechnung
########################## enthalten, sprich von letzten Parametern in paramDF (500, 8, 0.1 etc.)

# depth_leaves <- bind_cols(paramDF, bestResults)
# library(gdata)
# depth_leaves <- cbindX(paramDF, bestResults)
  
# depth_leaves[which.min(depth_leaves$test_rmse_mean),]

# save best model-parameter
# save(depth_leaves, file = "depth_leaves.rda")

# load model and data
load("depth_leaves.rda")

# show the best parameters (eta, max_depth etc.), where test values are at minimum
depth_leaves[which.min(depth_leaves$test_rmse_mean),] # or see rwCV.$best_iteration
```

```{r}
#| warning: FALSE
#| echo: False
#| message: False

# -------------------------- define XGBoost model --------------------------
# set.seed(100)
mod.xgb = xgb.train(data = xgb_train, watchlist=watchlist, verbose = 0, nrounds = 100, max_depth = 8, eta = 0.1, gamma = 0, colsample.bytree = 1, min_child_weight = 3, subsample = 1, lambda = 1, alpha = 0) # eta = learning_rate

#use model to make predictions on test data
pred_y = predict(mod.xgb, xgb_test)

# new column for estimated values
test.data$predict <- pred_y

# R^2
# (R2 = 1 - sum((test.data$N - test.data$predict)^2) / sum((test.data$N - mean(test.data$N))^2))
```

## Actual vs. estimated nitrogen content according to XGBOOST

The R² value of 0.54 is significantly higher compared to that of the MLR model (0.41), indicating a much better fit.

Of course, this can be further **improved** **by including additional process parameters**. **For the sake of simplicity, we have used only five parameters here.**

```{r}
#| warning: FALSE
#| echo: False
#| message: False
#| label: fig-ActualVsEstimatedXGBoost
#| fig-cap: Actual vs. estimated nitrogen content according to XGBoost
#| fig-align: "center"
#| fig-pos: "H"
#| fig-width: 10
#| fig-height: 7

# Plot Messung gegen Schätzung
ggplot(test.data, aes(x = predict, y = N)) +
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  scale_x_continuous(breaks = pretty_breaks(n = 10), limits = c(0.3, 0.46)) +
  scale_y_continuous(breaks = pretty_breaks(n = 10), limits = c(0.3, 0.46)) +
  theme_bw(base_size = 25) + annotate("text", x = 0.44, y = 0.32, label = "R^2 == 0.54", size = 8, parse = T) +
  labs(x='Berechneter N-Wert', y='Gemessener N-Wert', title='XGBOOST')
```

## Feature importance according to "Tree Shap"

The SHAP summary plot provides an excellent visualization for obtaining an overview of the effect sizes of the parameters (for more information to "SHAP for XGBoost in R" see [SHAPvalues](https://liuyanguu.github.io/post/2019/07/18/visualization-of-shap-for-xgboost/)).

Similar to the MLR model, Parameter_3 is the most important process parameter for controlling the nitrogen content in the **p**ressure-**e**lectro**s**lag-**r**emelting (PESR) furnace. As this parameter increases, the nitrogen content in the molten steel also increases.

```{r}
#| warning: FALSE
#| echo: False
#| message: False
#| label: fig-ShapFeatureSummary
#| fig-cap: Effects of individual parameters according to XGBOOST  
#| fig-align: "center"
#| fig-pos: "H"
#| fig-width: 10
#| fig-height: 8
# https://liuyanguu.github.io/post/2019/07/18/visualization-of-shap-for-xgboost/
# library(SHAPforxgboost)
# To return the SHAP values and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = mod.xgb, X_train = x_test)
# The ranked features by mean |SHAP|
# shap_values.ein$mean_shap_score

# To prepare the long-format data:
shap_long <- shap.prep(xgb_model = mod.xgb, X_train = x_test)
# is the same as: using given shap_contrib
shap_long <- shap.prep(shap_contrib = shap_values$shap_score, X_train = x_test)

# **SHAP summary plot**
#png("HBAlleFaktorenMnCr-Einsaztstähle.png", width = 1500, height = 800)
shap.plot.summary(shap.prep(xgb_model = mod.xgb, X_train = x_test)) + theme_bw(base_size = 20) + theme(legend.position = "bottom")
#dev.off()
## Median als rheihenfolge statt Mittelwert
# g <- shap_long %>% group_by(variable) %>%  mutate(mean_value = median(abs(value)))
# shap.plot.summary(g)
```

The influence of individual process parameters is shown in detail below. The red line indicates the curve progression according to the XGB model, and the points represent the measured SHAP values for each process parameter. For instance, as parameter_3 (x-axis) increases, the SHAP value (y-axis) - and consequently the nitrogen content in the steel - also increases.

```{r}
#| warning: FALSE
#| echo: False
#| message: False
#| label: fig-ShapFeatureprogression
#| fig-cap: Progression of effects of individual parameters according to XGBOOST  
#| fig-align: "center"
#| fig-pos: "H"
#| fig-width: 10
#| fig-height: 12

data.feature <-  shap.prep(mod.xgb, X_train = as.matrix(x_test))
shap.values.feature <- shap.values(mod.xgb, x_test)
features.ranked <- names(shap.values.feature$mean_shap_score)[1:5]
fig.list <- lapply(features.ranked, shap.plot.dependence, data_long = data.feature)
gridExtra::grid.arrange(grobs = fig.list, ncol = 2)
```

# Conclusion

The assessment of the **p**ressure-**e**lectro**s**lag-**r**emelting (PESR) process was carried out in this work using data analysis techniques, including multiple linear regression (MLR) and machine learning algorithms such as Extreme Gradient Boosting (XGB). We observed that modeling with Extreme Gradient Boosting (XGB) demonstrated superior performance and provided a more effective assessment of feature importance compared to MLR.

Data analysis is only one of the many methods we use at [ETE](https://www.gmh-gruppe.de/standorte/energietechnik-essen-gmbh/). In the next step, we will present how we optimize individual steel grades and processes through **simulation**.
